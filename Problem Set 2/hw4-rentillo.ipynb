{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03bb53d1",
   "metadata": {},
   "source": [
    "# CMSC 197 (Machine Learning) - Problem Set 2\n",
    "\n",
    "### Earl James Q. Rentillo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "cb2d6e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import preliminary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sklearn\n",
    "import sklearn.metrics\n",
    "import re\n",
    "import email\n",
    "from collections import Counter\n",
    "\n",
    "# import sklearn libraries\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2083ae98",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d58c765e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000/000</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000/001</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000/002</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000/003</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000/004</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000/005</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000/006</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000/007</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>000/008</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000/009</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>000/010</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>000/011</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>000/012</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>000/013</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>000/014</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   location label\n",
       "0   000/000   ham\n",
       "1   000/001  spam\n",
       "2   000/002  spam\n",
       "3   000/003   ham\n",
       "4   000/004  spam\n",
       "5   000/005   ham\n",
       "6   000/006   ham\n",
       "7   000/007  spam\n",
       "8   000/008  spam\n",
       "9   000/009  spam\n",
       "10  000/010   ham\n",
       "11  000/011  spam\n",
       "12  000/012  spam\n",
       "13  000/013  spam\n",
       "14  000/014  spam"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create emails dataframe\n",
    "emails = pd.DataFrame()\n",
    "labels = []\n",
    "locs = []\n",
    "\n",
    "# read labels file, split the label and file location, then append lists as part of the dataframe\n",
    "with open('labels') as f:\n",
    "    for line in f:\n",
    "        label, loc = line.split()\n",
    "        labels.append(label)\n",
    "        locs.append(loc.replace('../data/', ''))\n",
    "\n",
    "emails['location'] = locs\n",
    "emails['label'] = labels\n",
    "emails.head()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "300ec198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'able', 'about', 'above', 'abst']\n"
     ]
    }
   ],
   "source": [
    "# reading stopwords.txt\n",
    "\n",
    "with open('stop_words.txt', 'r') as f:\n",
    "    stopwords = f.readlines()\n",
    "    \n",
    "# remove '\\n'\n",
    "stopwords = [word[:-1] for word in stopwords]\n",
    "\n",
    "# check first 5 words\n",
    "print(stopwords[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7cbba93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining functions to get email body and clean an email\n",
    " \n",
    "def get_email_body(message):\n",
    "    msg = email.message_from_file(message)\n",
    "\n",
    "    # if the email is multipart\n",
    "    if msg.is_multipart():\n",
    "        for part in msg.walk():\n",
    "            # find part with the text/plain content type\n",
    "            if part.get_content_type() == 'text/plain':\n",
    "                # extract part as a string\n",
    "                body = part.get_payload()\n",
    "                return body\n",
    "\n",
    "    # if the message is single part\n",
    "    else:\n",
    "        body = msg.get_payload()\n",
    "        return body\n",
    "\n",
    "def clean_email(content):\n",
    "    # remove html tags, non-alphabets, and links\n",
    "    content = re.sub(r'<[^>]*>', '', content)\n",
    "    content = re.sub(r'[^a-zA-Z\\n ]','', content)\n",
    "    content = re.sub(r'http\\S+', '', content)\n",
    "    content = re.sub(r'www', '', content)\n",
    "    content = re.sub(r'goo\\S+', '', content)\n",
    "    content = content.lower().split()\n",
    "    \n",
    "    # remove stopwords in email content\n",
    "    content = [word for word in content if word not in stopwords]\n",
    "    \n",
    "    content = ' '.join(content)\n",
    "    # remove stop words and other unwanted words \n",
    "    unwanted_words = ['mime', 'mimeversion', 'contenttransferencoding', 'contenttransfer' , 'contenttype',\n",
    "                      'textplain', 'texthtml', 'charsetiso', 'formatflowed', 'multipart message' ]\n",
    "    for word in unwanted_words:\n",
    "        content = content.replace(word, '')\n",
    "   \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9f775766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the email files and cleaning \n",
    "\n",
    "contents = []\n",
    "\n",
    "for loc in emails['location']:\n",
    "\n",
    "    with open(f'data/{loc}', 'r', encoding = 'latin-1') as f:\n",
    "        content = clean_email(str(get_email_body(f)))\n",
    "        contents.append((content))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ceecd5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>label</th>\n",
       "      <th>email_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000/000</td>\n",
       "      <td>ham</td>\n",
       "      <td>mailing list queried weeks ago running set arc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000/001</td>\n",
       "      <td>spam</td>\n",
       "      <td>luxury watches buy rolex rolex cartier bvlgari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000/002</td>\n",
       "      <td>spam</td>\n",
       "      <td>academic qualifications prestigious nonacc red...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000/003</td>\n",
       "      <td>ham</td>\n",
       "      <td>greetings verify subscription planfans list ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000/004</td>\n",
       "      <td>spam</td>\n",
       "      <td>chauncey conferred luscious continued tonsillitis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location label                                      email_content\n",
       "0  000/000   ham  mailing list queried weeks ago running set arc...\n",
       "1  000/001  spam  luxury watches buy rolex rolex cartier bvlgari...\n",
       "2  000/002  spam  academic qualifications prestigious nonacc red...\n",
       "3  000/003   ham  greetings verify subscription planfans list ch...\n",
       "4  000/004  spam  chauncey conferred luscious continued tonsillitis"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails['email_content'] = contents\n",
    "emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5253d62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set length: 21300\n",
      "Test set length: 16522\n"
     ]
    }
   ],
   "source": [
    "# set emails in folders 0-70 to train set\n",
    "train_set = emails[emails['location'] < '071']\n",
    "\n",
    "# set emails in folders 71-127 to test set\n",
    "test_set = emails[emails['location'] >= '071']\n",
    "\n",
    "# check length of train and test set\n",
    "print(f'Train set length: {len(train_set)}')\n",
    "print(f'Test set length: {len(test_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b0b77b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the training set into ham and spam\n",
    "\n",
    "train_ham = emails[emails['label'] == 'ham'].reset_index()\n",
    "train_spam = emails[emails['label'] == 'spam'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "4faa0d8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>num_of_occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>will</td>\n",
       "      <td>11304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bb</td>\n",
       "      <td>7395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>board</td>\n",
       "      <td>5148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>company</td>\n",
       "      <td>4533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>price</td>\n",
       "      <td>4497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     words  num_of_occurences\n",
       "0     will              11304\n",
       "1       bb               7395\n",
       "2    board               5148\n",
       "3  company               4533\n",
       "4    price               4497"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the most common 10000 words from the training dataset\n",
    "most_used_words = Counter(\" \".join(train_set['email_content']).split()).most_common(10000)\n",
    "\n",
    "top_used_words = pd.DataFrame(most_used_words, columns = ['words', 'num_of_occurences'])\n",
    "top_used_words.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c8e7e4",
   "metadata": {},
   "source": [
    "## Creating the Feature Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f44fde35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the feature dictionary for ham and spam\n",
    "\n",
    "ham_word_counts = {unique_words: [0] * len(train_ham) for unique_words, _ in most_used_words}\n",
    "spam_word_counts = {unique_words: [0] * len(train_spam) for unique_words, _ in most_used_words}\n",
    "\n",
    "top_words_list = [key for key, _ in most_used_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "bb02cc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spam feature set\n",
    "spam_word_count = pd.DataFrame(spam_word_counts)\n",
    "\n",
    "# loop through the train spam set index\n",
    "for i in train_spam.index:\n",
    "    # count the word frequency per row in the train spam set\n",
    "    frequency = dict(Counter(train_spam['email_content'][i].split()))\n",
    "    for key, val in frequency.items():\n",
    "        if key in top_words_list:\n",
    "            spam_word_count.loc[i, key] += val\n",
    "\n",
    "spam_feat_matrix = spam_word_count.to_numpy()\n",
    "spam_feat_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8d747fca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ham feature set\n",
    "ham_word_count = pd.DataFrame(ham_word_counts)\n",
    "\n",
    "for i in train_ham.index:\n",
    "    frequency = dict(Counter(train_ham['email_content'][i].split()))\n",
    "    # count the word frequency per row in the train ham set\n",
    "    for key, val in frequency.items():\n",
    "        if key in top_words_list:  # add the word frequency to the row and column where the word is found\n",
    "            ham_word_count.loc[i, key] += val\n",
    "\n",
    "ham_feat_matrix = ham_word_count.to_numpy()\n",
    "ham_feat_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31a01d0",
   "metadata": {},
   "source": [
    "## Computing the ham and spam priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "20a6cc7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probabilities for spam:  1.1695774647887325\n",
      "Prior probabilities for ham:  0.6061032863849766\n"
     ]
    }
   ],
   "source": [
    "# number of spam emails\n",
    "spam_train_size = train_spam.shape[0]\n",
    "# number of ham emails\n",
    "ham_train_size = train_ham.shape[0]\n",
    "# total number of emails for training\n",
    "total_train_size = train_set.shape[0]      \n",
    "\n",
    "spam_prior =  spam_train_size / total_train_size\n",
    "ham_prior = ham_train_size / total_train_size\n",
    "\n",
    "print(\"Prior probabilities for spam: \", spam_prior)\n",
    "print(\"Prior probabilities for ham: \", ham_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6ac3f",
   "metadata": {},
   "source": [
    "## Computing the likelihood of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "187baa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of each words in spam\n",
    "spam_train_words_sum = np.sum(spam_feat_matrix, axis=0)\n",
    "# sum of each words in ham\n",
    "ham_train_words_sum = np.sum(ham_feat_matrix, axis=0)\n",
    "\n",
    "# total sum of words in spam\n",
    "spam_train_word_total = spam_train_words_sum.sum()\n",
    "# total sum of words in ham\n",
    "ham_train_word_total = ham_train_words_sum.sum()\n",
    "\n",
    "# laplace smoothing\n",
    "lambda_val = 1 \n",
    "ham_likelihood = {}\n",
    "spam_likelihood = {}\n",
    "\n",
    "for i in range(len(top_words_list)):\n",
    "    c_ham = (ham_train_words_sum[i]+lambda_val) / (ham_train_word_total + lambda_val*len(top_words_list))\n",
    "    c_spam = (spam_train_words_sum[i]+lambda_val) / (spam_train_word_total + lambda_val*len(top_words_list))\n",
    "    \n",
    "    ham_likelihood[top_words_list[i]] = c_ham\n",
    "    spam_likelihood[top_words_list[i]] = c_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68005c1f",
   "metadata": {},
   "source": [
    "## Classifying the emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "76035621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining function to classify emails\n",
    "\n",
    "def classify_emails(email_content, spam_prior, ham_prior, spam_likelihood, ham_likelihood, top_words_list):\n",
    "    \n",
    "    # get log values of spam and ham probabilities\n",
    "    spam_prob_log = np.log(spam_prior)\n",
    "    ham_prob_log = np.log(ham_prior)\n",
    "    \n",
    "    email_words = str(email_content).split()\n",
    "    \n",
    "    for word in email_words:\n",
    "        if word in top_words_list:\n",
    "            ham_prob_log += np.log(ham_likelihood[word])\n",
    "            spam_prob_log += np.log(spam_likelihood[word])\n",
    "            \n",
    "    if spam_prob_log > ham_prob_log:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'ham'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde1edf8",
   "metadata": {},
   "source": [
    "## Testing the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1685c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary with the predicted label and respective location\n",
    "\n",
    "predicted_dict = {'location':[], 'predicted_label': []}\n",
    "\n",
    "for path, content in zip(test_set['location'], test_set['email_content']):\n",
    "    predicted_dict['location'].append(path) \n",
    "    pred_label = classify_emails(content, spam_prior, ham_prior, spam_likelihood, ham_likelihood, top_words_list)\n",
    "    \n",
    "    #add the predicted label by the classfying_emails function\n",
    "    predicted_dict['predicted_label'].append(pred_label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d08c20a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>label</th>\n",
       "      <th>email_content</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>071/000</td>\n",
       "      <td>spam</td>\n",
       "      <td>hesitantly derive perverse satisfaction clodho...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>071/001</td>\n",
       "      <td>ham</td>\n",
       "      <td>things perform experiment display will remain ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>071/002</td>\n",
       "      <td>spam</td>\n",
       "      <td>best offer month viggra ci ialis vaiium xa naa...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>071/003</td>\n",
       "      <td>spam</td>\n",
       "      <td>de ar wne cr doesnt matter ow real st mmed ia ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>071/004</td>\n",
       "      <td>spam</td>\n",
       "      <td>special offer adobe video collection adobe pre...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location label                                      email_content  \\\n",
       "0  071/000  spam  hesitantly derive perverse satisfaction clodho...   \n",
       "1  071/001   ham  things perform experiment display will remain ...   \n",
       "2  071/002  spam  best offer month viggra ci ialis vaiium xa naa...   \n",
       "3  071/003  spam  de ar wne cr doesnt matter ow real st mmed ia ...   \n",
       "4  071/004  spam  special offer adobe video collection adobe pre...   \n",
       "\n",
       "  predicted_label  \n",
       "0            spam  \n",
       "1             ham  \n",
       "2            spam  \n",
       "3            spam  \n",
       "4            spam  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test = pd.DataFrame.from_dict(predicted_dict)\n",
    "\n",
    "merged_emails = pd.merge(test_set, predicted_test, on='location')\n",
    "merged_emails.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea1a75",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6291a2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Evaluation on Test Set\n",
      "\n",
      "Accuracy score:  0.9391720130734778\n",
      "Recall score: 0.9311180960933992\n",
      "Precision score: 0.9775598717706958\n",
      "F1 score: 0.9537739754381123\n"
     ]
    }
   ],
   "source": [
    "# performance evaluation for accuracy, recall, precision and f1 score\n",
    "\n",
    "actual_label = merged_emails['label'].to_numpy()\n",
    "predicted_label = merged_emails['predicted_label'].to_numpy()\n",
    "\n",
    "print('Performance Evaluation on Test Set\\n')\n",
    "print('Accuracy score: ', accuracy_score(actual_label, predicted_label))\n",
    "print(\"Recall score:\", recall_score(actual_label, predicted_label, pos_label=\"spam\"))\n",
    "print(\"Precision score:\", precision_score(actual_label, predicted_label, pos_label=\"spam\"))\n",
    "print(\"F1 score:\", f1_score(actual_label, predicted_label, pos_label=\"spam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "122e21c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10368,   767],\n",
       "       [  238,  5149]], dtype=int64)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "\n",
    "c_matrix = confusion_matrix(merged_emails['label'].to_numpyb(), merged_emails['predicted_label'].to_numpy(), labels=[\"spam\", \"ham\"])\n",
    "c_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
